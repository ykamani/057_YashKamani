# -*- coding: utf-8 -*-
"""Lab7_LogisticRegression_Tweets_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJBGbqic7VIVP8TXpwkOy576VS2hjgrJ
"""

import nltk
from nltk.corpus import twitter_samples
import pandas as pd
import tensorflow as tf

nltk.download('twitter_samples')
nltk.download('stopwords')

import re
import string
import numpy as np

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer

# process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.

def process_tweet(tweet):
    """Process tweet function.
    Input:
        tweet: a string containing a tweet
    Output:
        tweets_clean: a list of words containing the processed tweet

    """
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')

    # removing stock market tickers like $GE
    tweet = re.sub(r'\$\w*', '', tweet)
    
    # removing old style retweet text "RT"
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    
    # removing hyperlinks
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
    
    # removing hashtags
    tweet = re.sub(r'#', '', tweet)
    
    # tokenize tweets
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)

    tweets_clean = []
    for word in tweet_tokens:
        # 1 removing stopwords
        if word in stopwords_english:
            continue
        # 2 removing punctuation
        if word in string.punctuation:
            continue
        # 3 stemming word
        word = stemmer.stem(word)
        # 4 Add this to tweets_clean
        tweets_clean.append(word)

    return tweets_clean

# build_freqs(): counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0',
# then it builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.

def build_freqs(tweets, ys):
    """Build frequencies.
    Input:
        tweets: a list of tweets
        ys: an m x 1 array with the sentiment label of each tweet
            (either 0 or 1)
    Output:
        freqs: a dictionary mapping each (word, sentiment) pair to its
        frequency
    """
    # Converting np array to list since zip needs an iterable.
    # The squeeze is necessary or the list ends up with one element.
    yslist = np.squeeze(ys).tolist()

    # Starting with an empty dictionary and populate it by looping over all tweets and over all processed words in each tweet.
    freqs = {}

    for y, tweet in zip(yslist, tweets):
        for word in process_tweet(tweet):
            pair = (word, y)

            # Updating the count of pair if present, set it to 1 otherwise
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1

    return freqs

# selecting the set of positive and negative tweets

all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# spliting the data into two pieces, one for training and other is for testing

from sklearn.model_selection import train_test_split

train_pos, test_pos = train_test_split(all_positive_tweets, test_size=0.20, random_state = 57)
train_neg, test_neg = train_test_split(all_negative_tweets, test_size=0.20, random_state = 57)

train_x = train_pos + train_neg
test_x = test_pos + test_neg

# combine positive and negative labels

train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)
test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)

# creating frequency dictionary

freqs = build_freqs(train_x,train_y)

# verifying the output

print("type(freqs) = " + str(type(freqs)))
print("len(freqs) = " + str(len(freqs.keys())))

# Processing tweet example

print('Example of a positive tweet: \n', train_x[0])
print('\nExample of the processed  tweet: \n', process_tweet(train_x[0]))

# extract_features(): It tokenizes, stems, and removes stopwords from the tweets

def extract_features(tweet, freqs):
    '''
    Input:
        tweet: a list of words for one tweet
        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
    Output:
        x: a feature vector of dimension (1,3)
    '''
    
    output = []
    for word_l in tweet:
        word_l = process_tweet(word_l)

        # 3 elements in the form of a 1 x 3 vector
        x = np.zeros((1, 3))

        # bias term is set to 1
        x[0,0] = 1

        # looping through each word in the list of words
        for word in word_l:

            # incrementing the word count for the positive label 1
            x[0,1] += freqs.get((word, 1.0),0)

            # incrementing the word count for the negative label 0
            x[0,2] += freqs.get((word, 0.0),0)


        assert(x.shape == (1, 3))
        output.append(x)
    return output

model = tf.keras.models.Sequential([ tf.keras.layers.Dense( 2, activation=tf.nn.softmax ) ])

model.compile( optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'] )

model.fit(tf.convert_to_tensor( extract_features(train_x, freqs)), train_y, epochs=5 )

model.evaluate(tf.convert_to_tensor( extract_features(test_x, freqs)), test_y )