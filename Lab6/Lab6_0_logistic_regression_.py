# -*- coding: utf-8 -*-
"""0_logistic_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fcFeIlmd0oSgxSL-im4BfVvbaenAXAyF
"""

#Importing libraries
import numpy as np 
import pandas as pd 
import io
import matplotlib.pyplot as plt

# reading the csv file, del 2 columns from the file, checking first few rows of the file

data = pd.read_csv('BuyComputer.csv')

data.drop(columns=['User ID',],axis=1,inplace=True)
print(data[0:5])
data.shape

#Declare label as last column in the source file
y = data.iloc[:,-1].values
y[0:10]

#Declaring X as all columns excluding last
X = data.iloc[:,:-1].values
X[0:10]

# Splitting data
# Roll No. : 057
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 57)
print(X_train.shape)
print(y_train.shape)

# Sacaling data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Variabes to calculate sigmoid function
y_pred = []
len_x = len(X_train[0])
w = []
b = 0.2
print(len_x)
print(len(y_pred))

entries = len(X_train[:,0])
entries

for weights in range(len_x):
    w.append(0)
w

# Sigmoid function
def sigmoid(z):
  return (1/(1+np.exp(-z)))

def predict(inputs):
    z = np.dot(w,inputs)+b
    a = sigmoid(z)
    return a

#Loss function
def loss_func(y,a):
    J = -(y*np.log(a) + (1-y)*np.log(1-a))
    return J

dw = []
db = 0
J = 0
alpha = 0.1
for x in range(len_x):
    dw.append(0)

#Repeating the process 3000 times
for iterations in range(3000):
    for i in range(entries):
        localx = X_train[i]
        a = predict(localx)   
        dz = a - y_train[i]
        J += loss_func(y_train[i],a)
        for j in range(len_x):
            dw[j] = dw[j]+(localx[j]*dz)
        db += dz
    J = J/entries
    db = db/entries
    for x in range(len_x):
        dw[x]=dw[x]/entries
    for x in range(len_x):
        w[x] = w[x]-(alpha*dw[x])
    b = b-(alpha*db)         
    J=0

#Print weight
print(w)

#print bias
print(b)

#predicting the label
print(len(y_pred))
for x in range(len(y_test)):
    y_pred.append(predict(X_test[x]))
y_pred[0:10]

#print actual and predicted values in a table
print(len(y_pred))
for x in range(len(y_pred)-1):
    if y_pred[x]>=0.5:
        y_pred[x]=1
    else:
        y_pred[x]=0
    print('Actual ',y_test[x],' Predicted ',y_pred[x])

# Calculating accuracy of prediction
count = 0
for x in range(len(y_pred)):
    if(y_pred[x]==y_test[x]):
        count=count+1
print('Accuracy:',(count/(len(y_pred)))*100)

"""#Using sklearn LogisticRegression model"""

# Fitting Logistic Regression to the Training set
# Roll No. : 057
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(random_state = 57)

#Fit
LR.fit(X_train, y_train)

#predicting the test label with LR. Predict always takes X as input

print("Accuracy : ",LR.score(X_test,y_test),"\n")

y_predLR=LR.predict(X_test)

for x in range(len(y_pred)):
    print('Actual ',y_test[x],' Predicted ',y_predLR[x])