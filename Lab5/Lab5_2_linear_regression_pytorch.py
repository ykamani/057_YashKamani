# -*- coding: utf-8 -*-
"""2-linear-regression-pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uPpOxTMx8noC-jLi-zPn7mE4h98xkVjl
"""

# Import Numpy & PyTorch
import numpy as np
import torch

"""## Linear Regression Model using PyTorch built-ins

Let's re-implement the same model using some built-in functions and classes from PyTorch.

And now using two different targets: Apples and Oranges
"""

# Imports
import torch.nn as nn

# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype='float32')
# Targets (apples, oranges)
targets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119], 
                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119], 
                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype='float32')

inputs = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)

print(inputs)
print(targets)

"""### Dataset and DataLoader

We'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples. We'll also create a DataLoader, to split the data into batches while training. It also provides other utilities like shuffling and sampling.
"""

# Import tensor dataset & data loader
from torch.utils.data import TensorDataset, DataLoader

# Define dataset
#The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). 
#It returns a tuple (or pair), in which the first element contains the input variables for the selected rows, and the second contains the targets.
train_ds = TensorDataset(inputs, targets)
train_ds[0:5]

# Define data loader
# DataLoader, which can split the data into batches of a predefined size while training. 
# It also provides other utilities like shuffling and random sampling of the data
from torch.utils.data import DataLoader
batch_size = 5
train_dl = DataLoader(train_ds, batch_size, shuffle=True)

"""### nn.Linear
Instead of initializing the weights & biases manually, we can define the model using `nn.Linear`.
"""

# Define model
model = nn.Linear(3, 2)
print(model.weight)
print(model.bias)

"""### Optimizer
Instead of manually manipulating the weights & biases using gradients, we can use the optimizer `optim.SGD`.
"""

# Define optimizer
opt = torch.optim.SGD(model.parameters(), lr=1e-5)

"""### Loss Function
Instead of defining a loss function manually, we can use the built-in loss function `mse_loss`.
"""

# Import nn.functional
import torch.nn.functional as F

# Define loss function
loss_fn = F.mse_loss

loss = loss_fn(model(inputs), targets)
print(loss)

"""### Train the model

We are ready to train the model now. We can define a utility function `fit` which trains the model for a given number of epochs.
"""

# Define a utility function to train the model
def fit(num_epochs, model, loss_fn, opt):
  for epoch in range(num_epochs):
    for xb,yb in train_dl:
      # Generate predictions
      pred = model(xb)
      loss = loss_fn(pred, yb)
      # Perform gradient descent
      loss.backward()
      opt.step()
      opt.zero_grad()
    if (epoch+1) % 10 == 0:
      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Train the model for 100 epochs
fit(100, model, loss_fn, opt)

# Generate predictions
preds = model(inputs)
preds

# Compare with targets
targets

"""Now we can define the model, optimizer and loss function exactly as before."""

fit(100, model, loss_fn, opt)